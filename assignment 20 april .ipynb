{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3deff",
   "metadata": {},
   "source": [
    "# What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee96416",
   "metadata": {},
   "outputs": [],
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple machine learning algorithm that can be used for classification and regression tasks.\n",
    "It predicts the label or numerical value of a new data point based on the majority class or average value of its K nearest neighbors.\n",
    "The value of K is a hyperparameter that can be tuned to balance between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b8aa8",
   "metadata": {},
   "source": [
    "How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Cross-validation: Divide data into training and validation sets, try different values of K on validation set, select the K that performs best.\n",
    "\n",
    "2.Rule of thumb: Choose K as the square root of the total number of data points in the training set.\n",
    "\n",
    "3.Domain knowledge: Consider task-specific information to choose an appropriate value of K.\n",
    "\n",
    "4.Grid search: Try a range of values for K and evaluate the model's performance on a validation set for each value of K, select the K that performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedc245",
   "metadata": {},
   "source": [
    "# What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0281856",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Task: KNN classifier is used for classification tasks where the output variable is categorical, while KNN regressor is used for regression tasks where the output variable is continuous.\n",
    "\n",
    "2.Prediction: KNN classifier predicts the class label of a new data point based on the majority class of its K nearest neighbors, while KNN regressor predicts the numerical value of a new data point based on the average of the values of its K nearest neighbors.\n",
    "\n",
    "3.Evaluation: The performance of KNN classifier is evaluated using classification metrics such as accuracy, precision, recall, and F1-score, while the performance of KNN regressor is evaluated using regression metrics such as mean squared error, mean absolute error, and R-squared.\n",
    "\n",
    "4.Hyperparameters: Both KNN classifier and KNN regressor have a hyperparameter K, which represents the number of nearest neighbors to consider for making predictions. However, the optimal value of K may differ for classification and regression tasks, and may depend on the dataset and problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec4884",
   "metadata": {},
   "source": [
    "# How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The evaluation metrics for KNN depend on the task at hand, such as classification or regression.\n",
    "For classification tasks, common metrics include accuracy, precision, recall, and F1-score.\n",
    "For regression tasks, common metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared (R^2).\n",
    "A combination of these metrics may be used to evaluate the performance of KNN and select the best value of K.\n",
    "The choice of evaluation metric should be based on the problem domain and the goals of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5d2a6",
   "metadata": {},
   "source": [
    "# What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac91514",
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality is a phenomenon where the performance of many machine learning algorithms, including KNN, deteriorates as the number of features or dimensions in the dataset increases.\n",
    "As the number of dimensions increases, the volume of the feature space grows exponentially, resulting in a sparsity problem where the available data becomes increasingly sparse.\n",
    "This sparsity problem makes it difficult for KNN to find meaningful nearest neighbors and can lead to overfitting, high variance, and poor generalization performance.\n",
    "The curse of dimensionality can make it computationally expensive to perform nearest neighbor search in high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660c2c6",
   "metadata": {},
   "source": [
    "# How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c065308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deletion: One simple strategy is to delete any instances with missing values. However, this approach can result in loss of valuable information, especially if the missing values are from a significant proportion of the dataset.\n",
    "\n",
    "Imputation: Another strategy is to impute missing values with a substitute value. One way to impute missing values is to replace them with the mean, median, or mode of the available values in the corresponding feature. Another approach is to use more advanced imputation techniques such as k-nearest neighbor imputation or matrix completion methods.\n",
    "\n",
    "Treat missing values as a separate category: For categorical features, missing values can be treated as a separate category, which can be included in the distance calculation. For numerical features, missing values can be replaced with a large or small value that is outside the range of the available values, so that they will not affect the distance calculation.\n",
    "\n",
    "It is important to note that the choice of handling missing values can have a significant impact on the performance of the KNN algorithm, and the optimal strategy may depend on the dataset and problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba6bde",
   "metadata": {},
   "source": [
    "# Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21388124",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN classifier and KNN regressor are variants of the K-nearest neighbor algorithm used for classification and regression tasks, respectively.\n",
    "KNN classifier is suited for categorical output variables, while KNN regressor is suited for continuous output variables.\n",
    "Both models have hyperparameter K, evaluated with different metrics, and are affected by the curse of dimensionality.\n",
    "Choosing between them depends on the nature of the problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578b8e7",
   "metadata": {},
   "source": [
    "# What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "Strengths:\n",
    "\n",
    "Simple to implement and easy to understand.\n",
    "Non-parametric, meaning it makes no assumptions about the underlying data distribution.\n",
    "Can be effective in situations where the decision boundary is irregular or the data is noisy.\n",
    "\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Sensitive to the choice of the hyperparameter K and the distance metric used.\n",
    "Computationally expensive, particularly for large datasets or high-dimensional feature spaces.\n",
    "Performance can be affected by the curse of dimensionality, resulting in sparsity and overfitting.\n",
    "May not work well with imbalanced datasets or noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66327d",
   "metadata": {},
   "source": [
    "# What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean distance is the straight-line distance between two points in a Euclidean space, which is calculated by taking the square root of the sum of the squared differences between the corresponding feature values. It is sensitive to differences between all dimensions and is suitable for problems where all features are equally important.\n",
    "\n",
    "Manhattan distance, also known as taxicab distance or city block distance, is the sum of the absolute differences between the corresponding feature values of two data points. It is less sensitive to outliers and is suitable for problems where some features may be more important than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f53f8e",
   "metadata": {},
   "source": [
    "# What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature scaling is an important preprocessing step in KNN that helps to ensure that all features are equally important and contributes equally to the distance calculation.\n",
    "Since KNN uses distances between data points to make predictions, features with larger values can dominate the distance calculation and bias the results towards those features.\n",
    "Feature scaling can be used to normalize the range of feature values and bring them to a similar scale, which can improve the performance of KNN and prevent features with large values from dominating the distance calculation.\n",
    "Common methods of feature scaling include standardization and normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
